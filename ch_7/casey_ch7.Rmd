---
title: 'Eco-Detectives: chapter 7'
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: yes
    number_sections: false
    theme: cerulean
    highlight: haddock
    includes: 
      in_header: '~/github/src/templates/ohara_hdr.html'
  pdf_document:
    toc: true
---

``` {r setup, echo = TRUE, message = FALSE, warning = FALSE}

knitr::opts_chunk$set(fig.width = 6, fig.height = 4, fig.path = 'Figs/',
                      echo = TRUE, message = FALSE, warning = FALSE)

source('https://raw.githubusercontent.com/oharac/src/master/R/common.R')

set.seed(1)

```

$\newcommand{\E}{\mathbb{E}} \newcommand{\Var}{\mathrm{Var}}$ $\newcommand{\L}{\mathscr{L}} \newcommand{\LL}{\mathscr{l}}$

# Chapter 7: The Confrontation: Likelihood and Maximum Likelihood

Minimizing sum of squares requires no assumptions about the distribution of error/uncertainty.  But if we have an idea about the probability distribution of uncertainty, we can use the maximum likelihood method.  This allows us to put confidence bounds on parameters, which we cannot do with sum of squares.

Also, likelihood forms the basis of Bayesian analysis.


## Intro stuff

Start with a probability distribution, the probability of observing data $Y$ given a parameter $p$.  Consider a Poisson distribution for $k$ events in one unit of time:

$$\Pr \{Y | p\} = \Pr\{Y = k \hspace{5pt}| \text{ rate parameter } r\} = \frac{e^{-r}r^k}{k!}$$

This is probability of observation $Y$ given $r, k$, but we can also flip that to say the probability of $r, k$ given observation $y_i$.  For a set of data $Y$ the likelihood is the product of likelihoods of all the parameters given each observation.

$$\L\{r | y_1, ..., y_n \} = \prod_{i=1}^n \frac{e^{-r}r^{y_i}}{y_i!}$$

Easier to work with log likelihood, specifically negative log likelihood.

\begin{align*}
  \LL\{r | Y\} &= -\sum_{i=1}^n (-r \ln(e) + y_i \ln(r) - \ln(y_i!))\\
    &= nr - \ln(r) \sum_{i=1}^n y_i  + \sum_{i=1}^n \ln(y_i!)
\end{align*}

But since the last term is independent of $r$, we can ignore it (since we can't change its value by changing $r$; alternately if we take the derivative with respect to $r$ this term drops out).

\begin{align*}
  \LL\{r | Y\} &= nr - \ln(r) \sum_{i=1}^n y_i  + \sum_{i=1}^n \ln(y_i!)\\
  \frac{\partial\LL\{r | Y\}}{\partial r} &= n - \frac{\sum_{i=1}^n y_i}{r} = 0\\[12pt]
  \Longrightarrow \hat r &= \frac{\sum_{i=1}^n y_i}{n}
\end{align*}

## Pseudocode 7.1

Monte Carlo simulation of unfished population that becomes overharvested, then unharvested again as a management action.

Assume parameters $r = 0.5, K = 1000, \sigma_W = 0.1, \sigma_V = 0.1$.

* Set initial pop at $K$.
* Use logistic eq'n with process uncertainty to determine pop size for subsequent years; in years 3, 4, 5, harvest 50% of population.
* Calc *observed* pop in each time period.
* Repeat for 10 years.

``` {r} 
r <- 0.5
K <- 1000
sigma_W <- 0.1
sigma_V <- 0.1

n_years <- 10
N_vec <- vector(length = n_years + 1)
N_vec[1] <- K

for(i in 1:n_years) { ### i <- 1
  N_t <- N_vec[i]
  ### calc process uncertainty for this step
  W_t <- exp(rnorm(1, mean = 0, sd = sigma_W) - sigma_W^2 / 2)
  
  ### calc harvest for this step: 50% in years 3, 4, 5; 0 else
  C_t <- ifelse(i %in% 3:5, 0.5 * N_t, 0)
  
  N_vec[i + 1] <- W_t * (N_t + r * N_t * (1 - N_t / K) - C_t)
  
}

### Since observed count is independent of process uncertainty, we can just
### apply the observation uncertainty after the fact more easily.
V_vec <- exp(rnorm(length(N_vec), mean = 0, sd = sigma_V) - sigma_V^2 / 2)
N_obs <- N_vec * V_vec

df <- data.frame(year = 1:(n_years + 1),
                 actual   = N_vec,
                 observed = N_obs) %>%
  gather(type, N, -year)
ggplot(df, aes(x = year, y = N, color = type)) +
  ggtheme_plot() +
  xlim(c(0, NA)) + ylim(c(0, NA)) +
  geom_point() +
  geom_line()

```

## Pseudocode 7.2

Use a systematic search to identify "best fit" parameters to minimize negative log likelihood.

1. Input data values for observed pop size with only observation uncertainty, no process uncertainty.
2. For specified values of $r,K$ systematically search over individual $r,K$ values and generate predicted deterministic pop sizes using eq. 7.32.
    * $N_{t+1} = N_t + rN_t \left(1 - \frac{N_t}{K}\right) - C_t$
3. Calc the deviation at each time period using eq. 7.36.
4. Calc neg log-likelihood of deviations using eq. 7.38.
5. Sum $\l_t$ over $t$ to obtain neg log likelihood for this combination of $r, K$.
6. See which values of $r, K$ lead to smallest total log likelihood.

``` {r generate N vec with only obs uncertainty} 
r <- 0.5
K <- 1000
sigma_V <- 0.1

n_years <- 10
N_vec <- vector(length = n_years + 1)
N_vec[1] <- K

for(i in 1:n_years) { ### i <- 1
  N_t <- N_vec[i]

  ### calc harvest for this step: 50% in years 3, 4, 5; 0 else
  C_t <- ifelse(i %in% 3:5, 0.5 * N_t, 0)
  
  N_vec[i + 1] <- (N_t + r * N_t * (1 - N_t / K) - C_t)
  
}

### Since observed count is independent of process uncertainty, we can just
### apply the observation uncertainty after the fact more easily.
V_vec <- exp(rnorm(length(N_vec), mean = 0, sd = sigma_V) - sigma_V^2 / 2)
N_obs <- N_vec * V_vec

```

``` {r} 
r_vec <- seq(-.5, 1, 0.05)
K_vec <- seq(100, 5000, 100)
t_vec <- 1:10
sigma_W <- 0.1
sigma_V <- 0.1

n_years <- 10

### use N_obs from previous chunk
NLL_mat <- matrix(NA, nrow = length(r_vec), ncol = length(K_vec))

for(i in seq_along(r_vec)) { ### i <- 1
  r <- r_vec[i]
  for(j in seq_along(K_vec)) { ### j <- 1
    K <- K_vec[j]
    
    ### for this set of r, K, calculate the deterministic N time series
    N_vec <- vector(length = n_years + 1)
    N_vec[1] <- K
    for(k in 1:n_years) { ### i <- 1
      N_t <- N_vec[k]

      ### calc harvest for this step: 50% in years 3, 4, 5; 0.01 else
      C_t <- ifelse(k %in% 3:5, 0.5 * N_t, 0.01 * N_t)
      
      N_vec[k + 1] <- N_t + r * N_t * (1 - N_t / K) - C_t
    }    
    ### then calc the neg log likelihood
    
    d_vec <- log(N_obs) - log(N_vec)
    nll <- sum(log(sigma_V) + .5 * log(2* pi) + d_vec^2/(2 * sigma_V^2))
    
    NLL_mat[i, j] <- nll
  }
}

best_fit <- which(NLL_mat == min(NLL_mat), arr.ind = TRUE)
r_best <- r_vec[best_fit[1]]
K_best <- K_vec[best_fit[2]]

r_best; K_best
```

``` {r}

N_vec <- vector(length = n_years + 1)
N_vec[1] <- K_best
for(k in 1:n_years) { ### i <- 1
  N_t <- N_vec[k]

  ### calc harvest for this step: 50% in years 3, 4, 5; 0.01 else
  C_t <- ifelse(k %in% 3:5, 0.5 * N_t, 0.01 * N_t)
  
  N_vec[k + 1] <- N_t + r_best * N_t * (1 - N_t / K_best) - C_t
}

df <- data.frame(t = 1:(n_years + 1),
                 observed = N_obs,
                 deterministic = N_vec) %>%
  gather(type, N, -t)

ggplot(df, aes(x = t, y = N, color = type)) +
  geom_line() +
  geom_point() +
  ggtheme_plot() +
  xlim(0, 11) + ylim(0, NA)
```

## Pseudocode 7.3

Use a systematic search to identify "best fit" parameters to minimize negative log likelihood.

1. Input data values for observed pop size with only observation uncertainty, no process uncertainty.
2. For specified values of $r,K$ systematically search over individual $r,K$ values and generate predicted deterministic pop sizes using eq. 7.39.
    * $N_{t+1} = W_t \{ N_t + rN_t \left(1 - \frac{N_t}{K}\right) - C_t \}$
3. Calc the deviation at each time period using eq. 7.36.
4. Calc neg log-likelihood of deviations using eq. 7.38.
5. Sum $\l_t$ over $t$ to obtain neg log likelihood for this combination of $r, K$.
6. See which values of $r, K$ lead to smallest total log likelihood.

``` {r generate N vec with only process uncertainty} 
r <- 0.5
K <- 1000
sigma_V <- 0.1

n_years <- 10
N_vec <- vector(length = n_years + 1)
N_vec[1] <- K

for(i in 1:n_years) { ### i <- 1
  N_t <- N_vec[i]

  W_t <- exp(rnorm(1, mean = 0, sd = sigma_W) - sigma_W^2 / 2)
  
  C_t <- ifelse(i %in% 3:5, 0.5 * N_t, 0)
  
  N_vec[i + 1] <- W_t * (N_t + r * N_t * (1 - N_t / K) - C_t)
  
}

N_obs <- N_vec ### no obs uncertainty

```

``` {r} 
r_vec <- seq(-.5, 1, 0.05)
K_vec <- seq(100, 5000, 100)
t_vec <- 1:10

n_years <- 10

### use N_obs from previous chunk
NLL_mat <- matrix(NA, nrow = length(r_vec), ncol = length(K_vec))

for(i in seq_along(r_vec)) { ### i <- 1
  r <- r_vec[i]
  for(j in seq_along(K_vec)) { ### j <- 1
    K <- K_vec[j]
    
    ### for this set of r, K, calculate the deterministic N time series
    N_vec <- vector(length = n_years + 1)
    N_vec[1] <- K
    for(k in 1:n_years) { ### i <- 1
      N_t <- N_vec[k]

      ### calc harvest for this step: 50% in years 3, 4, 5; 0.01 else
      C_t <- ifelse(k %in% 3:5, 0.5 * N_t, 0.01 * N_t)
      
      N_vec[k + 1] <- N_t + r * N_t * (1 - N_t / K) - C_t
    }    
    ### then calc the neg log likelihood
    
    d_vec <- log(N_obs) - log(N_vec)
    nll <- sum(log(sigma_W) + .5 * log(2* pi) + d_vec^2/(2 * sigma_W^2))
    
    NLL_mat[i, j] <- nll
  }
}

best_fit <- which(NLL_mat == min(NLL_mat), arr.ind = TRUE)
r_best <- r_vec[best_fit[1]]
K_best <- K_vec[best_fit[2]]

r_best; K_best
```

``` {r 7.3}

N_vec <- vector(length = n_years + 1)
N_vec[1] <- K_best
for(k in 1:n_years) { ### i <- 1
  N_t <- N_vec[k]

  ### calc harvest for this step: 50% in years 3, 4, 5; 0.01 else
  C_t <- ifelse(k %in% 3:5, 0.5 * N_t, 0.01 * N_t)
  
  N_vec[k + 1] <- N_t + r_best * N_t * (1 - N_t / K_best) - C_t
}

df <- data.frame(t = 1:(n_years + 1),
                 observed = N_obs,
                 deterministic = N_vec) %>%
  gather(type, N, -t)

ggplot(df, aes(x = t, y = N, color = type)) +
  geom_line() +
  geom_point() +
  ggtheme_plot() +
  xlim(0, 11) + ylim(0, NA)
```

## Pseudocode 7.4

``` {r 7.4}
q <- 1
r <- 0.03
p <- -3
lambda <- 3

df <- data.frame(D = 1:20) %>%
  rowwise() %>%
  mutate(I_det = max(0, (p + q * D) / (1 + r * D))) %>%
  ungroup() %>%
  mutate(obs = rpois(n(), I_det))

mdl <- lm(obs ~ D, data = df)

obs_plot <- ggplot(df, aes(x = D, y = obs)) +
  ggtheme_plot() +
  geom_abline(intercept = mdl$coefficients[1], slope = mdl$coefficients[2], color = 'blue') +
  geom_line(aes(y = I_det), color = 'red') +
  geom_point(size = 2) +
  labs(x = 'True abundance',
       y = 'Observed')

obs_plot
```

## Pseudocode 7.5

To calc neg log likelihood of a given observation, start with likelihood $\L$:

$$\L(\lambda | k = obs) = P(k = obs | \lambda) = \frac{\lambda^{k} e^{-\lambda}}{k!}$$
then neg log likelihood:
$$\LL\{\lambda | k\} = -(-\lambda \ln(e) + k \ln(\lambda) - \ln(k!))$$
$$= \lambda - k\ln(\lambda)  + \ln(k!)$$
In this, $\lambda$ is our predicted value $I_{det}$ for each model.  Since the last term is independent of $\lambda$, we could just drop it - it is the same for each model, so drops out in the math later because we're looking at differences between the models.  But we can just calculate it and keep it in, why not?

``` {r 7.5}

q <- 1
r <- 0.03
p <- -3
lambda <- 3

nsims <- 1000

best_model <- vector(length = nsims)

for(sim in 1:nsims) { ### sim <- 1
  
  ### to avoid singular results, let's use a while loop, creating a
  ### new set of observations until the nls() models are NOT singular
  singular <- TRUE
  while(singular) {
    ### generate a new random set of observations
    obs_df <- data.frame(D = 1:20) %>%
      rowwise() %>%
      mutate(I_det = max(0, (p + q * D) / (1 + r * D))) %>%
      ungroup() %>%
      mutate(obs = rpois(n(), I_det))
    
    ### generate params for each model
    singular <- FALSE
    mdl_a_ls <- lm(obs ~ 0 + D, data = obs_df) %>% coefficients
    mdl_b_ls <- lm(obs ~ 1 + D, data = obs_df) %>% coefficients
    mdl_c_ls <- tryCatch(expr = {nls(obs ~ (a * D)/(1 + b * D), 
                                      start = c('a' = p, 'b' = r), 
                                      data = obs_df) %>% coefficients},
             error = function(e) TRUE)
    mdl_d_ls <- tryCatch(expr = {nls(obs ~ (w + a * D)/(1 + b * D),
                                     start = c('w' = p, 'a' = q, 'b' = r),
                                     data = obs_df) %>% coefficients},
             error = function(e) TRUE)
    singular <- any(mdl_c_ls == TRUE, mdl_d_ls == TRUE)
  }
  ### generate df for each model
  mdl_a_df <- data.frame(D = 1:20) %>%
    rowwise() %>%
    mutate(I_det = max(0, mdl_a_ls[1] * D),
           model = 'A') %>%
    ungroup()
  mdl_b_df <- data.frame(D = 1:20) %>%
    rowwise() %>%
    mutate(I_det = max(0, mdl_b_ls[1] + mdl_b_ls[2] * D),
           model = 'B') %>%
    ungroup()
  mdl_c_df <- data.frame(D = 1:20) %>%
    rowwise() %>%
    mutate(I_det = max(0, mdl_c_ls[1] * D / (1 + mdl_c_ls[2] * D)),
           model = 'C') %>%
    ungroup()
  mdl_d_df <- data.frame(D = 1:20) %>%
    rowwise() %>%
    mutate(I_det = max(0, (mdl_d_ls[1] + mdl_d_ls[2] * D) / (1 + mdl_d_ls[3] * D)),
           model = 'D') %>%
    ungroup()
  
  ### bind models and add column of observed values
  mdl_df <- bind_rows(mdl_a_df, mdl_b_df, mdl_c_df, mdl_d_df) %>%
    mutate(obs = rep(obs_df$obs, 4)) %>%
    mutate(nll = I_det - obs * log(I_det) + log(factorial(obs)) )
  
  mdl_compare <- mdl_df %>%
    group_by(model) %>%
    summarize(sum_nll = sum(nll, na.rm = TRUE))
  nll_vec <- mdl_compare$sum_nll %>% setNames(mdl_compare$model)
  
  ### a little testing to figure out which model is the best... ugh
  a_test <- c('B' = 2 * (nll_vec['A'] - nll_vec['B']) < qchisq(.95, df = 1), ### 1 dof
              'C' = 2 * (nll_vec['A'] - nll_vec['C']) < qchisq(.95, df = 1), ### 1 dof
              'D' = 2 * (nll_vec['A'] - nll_vec['D']) < qchisq(.95, df = 2)) ### 2 dof
  b_test <- c('C' = nll_vec['B'] < nll_vec['C'],
              'D' = 2 * (nll_vec['B'] - nll_vec['D']) < qchisq(.95, df = 1)) ### 1 dof
  c_test <- 2 * (nll_vec['C'] - nll_vec['D']) < qchisq(.95, df = 1) ### 1 dof
  best_model[sim] <- case_when(all(a_test) ~ 'A',
                               all(b_test) ~ 'B',
                               all(c_test) ~ 'C',
                               TRUE        ~ 'D')
}

table(best_model)

```

## Some code ideas to try later

``` {r, eval = FALSE}
# r_K_t_df<- tidyr::crossing(r = r_vec, K = K_vec, t = t_vec)

### https://stackoverflow.com/questions/38983532/dplyr-iterative-calculation
### In general, if you want to calculate values that rely on previous values, 
### you are better off using Reduce. here's an example with your data
### 
### x %>% mutate(x3 = Reduce(function(a,b) a*1.1+1, 1:(n()-1), 0, acc=T))
### But in your example, there is a closed form for the term that doesn't 
### rely on iteration. You can do
### 
### x %>% mutate(x4=(1.1^(row_number()-1)-1)/(1.1-1)*1)


# x <- data.frame(x1 = c(1:10)) 
# 
# #This works.
# x$x2[1] <- 0
# 
# for (i in 2:nrow(x)) {
#   x$x2[i] <- x$x2[i-1]*1.1 + 1
# }
# 
# x <- x %>% mutate(x3 = Reduce(function(a, b) a*1.1+1, 1:(n()-1), 0, acc=T))


# library(dplyr)
# library(magrittr)
# x <- data.frame(x1 = c(1:10))
# f_x <- function(x){(x-1)*1.1+1}
# x$x2 <-x %$% x1 %>% f_x
#     
#     
# N_det <- function(N_lag, r, K, harvest) {
#   N = N_lag + r * N_lag * (1 - N_lag / K) - harvest * N_lag
# }
# 
# N_df <- r_K_t_df %>%
#   group_by(r, K) %>%
#   mutate(harvest = ifelse(t %in% 3:5, 0.50, 0.01)) %>%
#   mutate(N = Map(f = N_det, r, K, harvest))



```


