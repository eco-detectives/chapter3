---
title: 'Eco-Detectives: chapter 7'
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: yes
    number_sections: true
    theme: cerulean
    highlight: haddock
    includes: 
      in_header: '~/github/src/templates/ohara_hdr.html'
  pdf_document:
    toc: true
---

``` {r setup, echo = TRUE, message = FALSE, warning = FALSE}

knitr::opts_chunk$set(fig.width = 6, fig.height = 4, fig.path = 'Figs/',
                      echo = TRUE, message = FALSE, warning = FALSE)

source('https://raw.githubusercontent.com/oharac/src/master/R/common.R')

```

$\newcommand{\E}{\mathbb{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\L}{\mathscr{L}} \newcommand{\LL}{\mathscr{l}}$

# Chapter 7: The Confrontation: Likelihood and Maximum Likelihood

Minimizing sum of squares requires no assumptions about the distribution of error/uncertainty.  But if we have an idea about the probability distribution of uncertainty, we can use the maximum likelihood method.  This allows us to put confidence bounds on parameters, which we cannot do with sum of squares.

Also, likelihood forms the basis of Bayesian analysis.


## Intro stuff

Start with a probability distribution, the probability of observing data $Y$ given a parameter $p$.  Consider a Poisson distribution for $k$ events in one unit of time:

$$\Pr \{Y | p\} = \Pr\{Y = k \hspace{5pt}| \text{ rate parameter } r\} = \frac{e^{-r}r^k}{k!}$$

This is probability of observation $Y$ given $r, k$, but we can also flip that to say the probability of $r, k$ given observation $y_i$.  For a set of data $Y$ the likelihood is the product of likelihoods of all the parameters given each observation.

$$\L\{r | y_1, ..., y_n \} = \prod_{i=1}^n \frac{e^{-r}r^{y_i}}{y_i!}$$

Easier to work with log likelihood, specifically negative log likelihood.

\begin{align*}
  \LL\{r | Y\} &= -\sum_{i=1}^n (-r \ln(e) + y_i \ln(r) - \ln(y_i!))\\
    &= nr - \ln(r) \sum_{i=1}^n y_i  + \sum_{i=1}^n \ln(y_i!)
\end{align*}

But since the last term is independent of $r$, we can ignore it (since we can't change its value by changing $r$; alternately if we take the derivative with respect to $r$ this term drops out).

\begin{align*}
  \LL\{r | Y\} &= nr - \ln(r) \sum_{i=1}^n y_i  + \sum_{i=1}^n \ln(y_i!)\\
  \frac{\partial\LL\{r | Y\}}{\partial r} &= n - \frac{\sum_{i=1}^n y_i}{r} = 0\\[12pt]
  \Longrightarrow \hat r &= \frac{\sum_{i=1}^n y_i}{n}
\end{align*}

## Pseudocode 7.1

Monte Carlo simulation of unfished population that becomes overharvested, then unharvested again as a management action.

Assume parameters $r = 0.5, K = 1000, \sigma_W = 0.1, \sigma_V = 0.1$.

* Set initial pop at $K$.
* Use logistic eq'n with process uncertainty to determine pop size for subsequent years; in years 3, 4, 5, harvest 50% of population.
* Calc *observed* pop in each time period.
* Repeat for 10 years.

``` {r} 
r <- 0.5
K <- 1000
sigma_W <- 0.1
sigma_V <- 0.1

n_years <- 10
N_vec <- vector(length = n_years + 1)
N_vec[1] <- K

for(i in 1:n_years) { ### i <- 1
  N_t <- N_vec[i]
  ### calc process uncertainty for this step
  W_t <- exp(rnorm(1, mean = 0, sd = sigma_W) - sigma_W^2 / 2)
  
  ### calc harvest for this step: 50% in years 3, 4, 5; 0 else
  C_t <- ifelse(i %in% 3:5, 0.5 * N_t, 0)
  
  N_vec[i + 1] <- W_t * (N_t + r * N_t * (1 - N_t / K) - C_t)
  
}

### Since observed count is independent of process uncertainty, we can just
### apply the observation uncertainty after the fact more easily.
V_vec <- exp(rnorm(length(N_vec), mean = 0, sd = sigma_V) - sigma_V^2 / 2)
N_obs <- N_vec * V_vec

df <- data.frame(year = 1:(n_years + 1),
                 actual   = N_vec,
                 observed = N_obs) %>%
  gather(type, N, -year)
ggplot(df, aes(x = year, y = N, color = type)) +
  ggtheme_plot() +
  xlim(c(0, NA)) + ylim(c(0, NA)) +
  geom_point() +
  geom_line()

```

