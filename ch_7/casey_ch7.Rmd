---
title: 'Eco-Detectives: chapter 7'
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
output: 
  html_document:
    code_folding: hide
    toc: FALSE
    toc_depth: 3
    toc_float: yes
    number_sections: false
    theme: cerulean
    highlight: haddock
    includes: 
      in_header: '~/github/src/templates/ohara_hdr.html'
  pdf_document:
    toc: true
---

``` {r setup, echo = TRUE, message = FALSE, warning = FALSE}

knitr::opts_chunk$set(fig.width = 6, fig.height = 4, fig.path = 'Figs/',
                      echo = TRUE, message = FALSE, warning = FALSE)

source('https://raw.githubusercontent.com/oharac/src/master/R/common.R')

```

$\newcommand{\E}{\mathbb{E}} \newcommand{\Var}{\mathrm{Var}}$ $\newcommand{\L}{\mathscr{L}} \newcommand{\LL}{\mathscr{l}}$

# Chapter 7: The Confrontation: Likelihood and Maximum Likelihood{.tabset}

Minimizing sum of squares requires no assumptions about the distribution of error/uncertainty.  But if we have an idea about the probability distribution of uncertainty, we can use the maximum likelihood method.  This allows us to put confidence bounds on parameters, which we cannot do with sum of squares.

Also, likelihood forms the basis of Bayesian analysis.


## Intro stuff

Start with a probability distribution, the probability of observing data $Y$ given a parameter $p$.  Consider a Poisson distribution for $k$ events in one unit of time:

$$\Pr \{Y | p\} = \Pr\{Y = k \hspace{5pt}| \text{ rate parameter } r\} = \frac{e^{-r}r^k}{k!}$$

This is probability of observation $Y$ given $r, k$, but we can also flip that to say the probability of $r, k$ given observation $y_i$.  For a set of data $Y$ the likelihood is the product of likelihoods of all the parameters given each observation.

$$\L\{r | y_1, ..., y_n \} = \prod_{i=1}^n \frac{e^{-r}r^{y_i}}{y_i!}$$

Easier to work with log likelihood, specifically negative log likelihood.

\begin{align*}
  \LL\{r | Y\} &= -\sum_{i=1}^n (-r \ln(e) + y_i \ln(r) - \ln(y_i!))\\
    &= nr - \ln(r) \sum_{i=1}^n y_i  + \sum_{i=1}^n \ln(y_i!)
\end{align*}

But since the last term is independent of $r$, we can ignore it (since we can't change its value by changing $r$; alternately if we take the derivative with respect to $r$ this term drops out).

\begin{align*}
  \LL\{r | Y\} &= nr - \ln(r) \sum_{i=1}^n y_i  + \sum_{i=1}^n \ln(y_i!)\\
  \frac{\partial\LL\{r | Y\}}{\partial r} &= n - \frac{\sum_{i=1}^n y_i}{r} = 0\\[12pt]
  \Longrightarrow \hat r &= \frac{\sum_{i=1}^n y_i}{n}
\end{align*}

## Pseudocode 7.1

Monte Carlo simulation of unfished population that becomes overharvested, then unharvested again as a management action.

Assume parameters $r = 0.5, K = 1000, \sigma_W = 0.1, \sigma_V = 0.1$.

* Set initial pop at $K$.
* Use logistic eq'n with process uncertainty to determine pop size for subsequent years; in years 3, 4, 5, harvest 50% of population.
* Calc *observed* pop in each time period.
* Repeat for 10 years.

``` {r} 
r <- 0.5
K <- 1000
sigma_W <- 0.1
sigma_V <- 0.1

n_years <- 10
N_vec <- vector(length = n_years + 1)
N_vec[1] <- K

for(i in 1:n_years) { ### i <- 1
  N_t <- N_vec[i]
  ### calc process uncertainty for this step
  W_t <- exp(rnorm(1, mean = 0, sd = sigma_W) - sigma_W^2 / 2)
  
  ### calc harvest for this step: 50% in years 3, 4, 5; 0 else
  C_t <- ifelse(i %in% 3:5, 0.5 * N_t, 0)
  
  N_vec[i + 1] <- W_t * (N_t + r * N_t * (1 - N_t / K) - C_t)
  
}

### Since observed count is independent of process uncertainty, we can just
### apply the observation uncertainty after the fact more easily.
V_vec <- exp(rnorm(length(N_vec), mean = 0, sd = sigma_V) - sigma_V^2 / 2)
N_obs <- N_vec * V_vec

df <- data.frame(year = 1:(n_years + 1),
                 actual   = N_vec,
                 observed = N_obs) %>%
  gather(type, N, -year)
ggplot(df, aes(x = year, y = N, color = type)) +
  ggtheme_plot() +
  xlim(c(0, NA)) + ylim(c(0, NA)) +
  geom_point() +
  geom_line()

```

## Pseudocode 7.2

Use a systematic search to identify "best fit" parameters to minimize negative log likelihood.

1. Input data values for observed pop size with only observation uncertainty, no process uncertainty.
2. For specified values of $r,K$ systematically search over individual $r,K$ values and generate predicted deterministic pop sizes using eq. 7.32.
    * $N_{t+1} = N_t + rN_t \left(1 - \frac{N_t}{K}\right) - C_t$
3. Calc the deviation at each time period using eq. 7.36.
4. Calc neg log-likelihood of deviations using eq. 7.38.
5. Sum $\l_t$ over $t$ to obtain neg log likelihood for this combination of $r, K$.
6. See which values of $r, K$ lead to smallest total log likelihood.

``` {r generate N vec with only obs uncertainty} 
r <- 0.5
K <- 1000
sigma_V <- 0.1

n_years <- 10
N_vec <- vector(length = n_years + 1)
N_vec[1] <- K

for(i in 1:n_years) { ### i <- 1
  N_t <- N_vec[i]

  ### calc harvest for this step: 50% in years 3, 4, 5; 0 else
  C_t <- ifelse(i %in% 3:5, 0.5 * N_t, 0)
  
  N_vec[i + 1] <- (N_t + r * N_t * (1 - N_t / K) - C_t)
  
}

### Since observed count is independent of process uncertainty, we can just
### apply the observation uncertainty after the fact more easily.
V_vec <- exp(rnorm(length(N_vec), mean = 0, sd = sigma_V) - sigma_V^2 / 2)
N_obs <- N_vec * V_vec

```

``` {r} 
r_vec <- seq(-.5, 1, 0.05)
K_vec <- seq(100, 5000, 100)
t_vec <- 1:10
sigma_W <- 0.1
sigma_V <- 0.1

n_years <- 10

### use N_obs from previous chunk
NLL_mat <- matrix(NA, nrow = length(r_vec), ncol = length(K_vec))

for(i in seq_along(r_vec)) { ### i <- 1
  r <- r_vec[i]
  for(j in seq_along(K_vec)) { ### j <- 1
    K <- K_vec[j]
    
    ### for this set of r, K, calculate the deterministic N time series
    N_vec <- vector(length = n_years + 1)
    N_vec[1] <- K
    for(k in 1:n_years) { ### i <- 1
      N_t <- N_vec[k]

      ### calc harvest for this step: 50% in years 3, 4, 5; 0.01 else
      C_t <- ifelse(k %in% 3:5, 0.5 * N_t, 0.01 * N_t)
      
      N_vec[k + 1] <- N_t + r * N_t * (1 - N_t / K) - C_t
    }    
    ### then calc the neg log likelihood
    
    d_vec <- log(N_obs) - log(N_vec)
    nll <- sum(log(sigma_V) + .5 * log(2* pi) + d_vec^2/(2 * sigma_V^2))
    
    NLL_mat[i, j] <- nll
  }
}

best_fit <- which(NLL_mat == min(NLL_mat), arr.ind = TRUE)
r_best <- r_vec[best_fit[1]]
K_best <- K_vec[best_fit[2]]

r_best; K_best
```

``` {r}

N_vec <- vector(length = n_years + 1)
N_vec[1] <- K_best
for(k in 1:n_years) { ### i <- 1
  N_t <- N_vec[k]

  ### calc harvest for this step: 50% in years 3, 4, 5; 0.01 else
  C_t <- ifelse(k %in% 3:5, 0.5 * N_t, 0.01 * N_t)
  
  N_vec[k + 1] <- N_t + r_best * N_t * (1 - N_t / K_best) - C_t
}

df <- data.frame(t = 1:(n_years + 1),
                 observed = N_obs,
                 deterministic = N_vec) %>%
  gather(type, N, -t)

ggplot(df, aes(x = t, y = N, color = type)) +
  geom_line() +
  geom_point() +
  ggtheme_plot() +
  xlim(0, 11) + ylim(0, NA)
```

## Pseudocode 7.3

Use a systematic search to identify "best fit" parameters to minimize negative log likelihood.

1. Input data values for observed pop size with only observation uncertainty, no process uncertainty.
2. For specified values of $r,K$ systematically search over individual $r,K$ values and generate predicted deterministic pop sizes using eq. 7.39.
    * $N_{t+1} = W_t \{ N_t + rN_t \left(1 - \frac{N_t}{K}\right) - C_t \}$
3. Calc the deviation at each time period using eq. 7.36.
4. Calc neg log-likelihood of deviations using eq. 7.38.
5. Sum $\l_t$ over $t$ to obtain neg log likelihood for this combination of $r, K$.
6. See which values of $r, K$ lead to smallest total log likelihood.

``` {r generate N vec with only process uncertainty} 
r <- 0.5
K <- 1000
sigma_V <- 0.1

n_years <- 10
N_vec <- vector(length = n_years + 1)
N_vec[1] <- K

for(i in 1:n_years) { ### i <- 1
  N_t <- N_vec[i]

  W_t <- exp(rnorm(1, mean = 0, sd = sigma_W) - sigma_W^2 / 2)
  
  C_t <- ifelse(i %in% 3:5, 0.5 * N_t, 0)
  
  N_vec[i + 1] <- W_t * (N_t + r * N_t * (1 - N_t / K) - C_t)
  
}

N_obs <- N_vec ### no obs uncertainty

```

``` {r} 
r_vec <- seq(-.5, 1, 0.05)
K_vec <- seq(100, 5000, 100)
t_vec <- 1:10

n_years <- 10

### use N_obs from previous chunk
NLL_mat <- matrix(NA, nrow = length(r_vec), ncol = length(K_vec))

for(i in seq_along(r_vec)) { ### i <- 1
  r <- r_vec[i]
  for(j in seq_along(K_vec)) { ### j <- 1
    K <- K_vec[j]
    
    ### for this set of r, K, calculate the deterministic N time series
    N_vec <- vector(length = n_years + 1)
    N_vec[1] <- K
    for(k in 1:n_years) { ### i <- 1
      N_t <- N_vec[k]

      ### calc harvest for this step: 50% in years 3, 4, 5; 0.01 else
      C_t <- ifelse(k %in% 3:5, 0.5 * N_t, 0.01 * N_t)
      
      N_vec[k + 1] <- N_t + r * N_t * (1 - N_t / K) - C_t
    }    
    ### then calc the neg log likelihood
    
    d_vec <- log(N_obs) - log(N_vec)
    nll <- sum(log(sigma_W) + .5 * log(2* pi) + d_vec^2/(2 * sigma_W^2))
    
    NLL_mat[i, j] <- nll
  }
}

best_fit <- which(NLL_mat == min(NLL_mat), arr.ind = TRUE)
r_best <- r_vec[best_fit[1]]
K_best <- K_vec[best_fit[2]]

r_best; K_best
```

``` {r 7.3}

N_vec <- vector(length = n_years + 1)
N_vec[1] <- K_best
for(k in 1:n_years) { ### i <- 1
  N_t <- N_vec[k]

  ### calc harvest for this step: 50% in years 3, 4, 5; 0.01 else
  C_t <- ifelse(k %in% 3:5, 0.5 * N_t, 0.01 * N_t)
  
  N_vec[k + 1] <- N_t + r_best * N_t * (1 - N_t / K_best) - C_t
}

df <- data.frame(t = 1:(n_years + 1),
                 observed = N_obs,
                 deterministic = N_vec) %>%
  gather(type, N, -t)

ggplot(df, aes(x = t, y = N, color = type)) +
  geom_line() +
  geom_point() +
  ggtheme_plot() +
  xlim(0, 11) + ylim(0, NA)
```

## Pseudocode 7.4

Ecological scenario of observing animals and inferring the true abundance from observations with error.  Use an index

$$I = \max\left(0, \frac{p + qD}{1 + rD}\right) \hspace{20pt} (\text{eqn }7.43)$$
where $q$ is a proportionality constant - observed animals is proportional to abundance; $p$ allows for possibility that we may falsely observe the target species even if not present ($p > 0$) or miss observing target species even when present ($p < 0$); and $r$ allows for non-linearity between index and true abundance.

``` {r 7.4}
q <- 1
r <- 0.03
p <- -3

df <- data.frame(D = 1:20) %>%
  rowwise() %>%
  mutate(I_det = max(0, (p + q * D) / (1 + r * D))) %>%
  ungroup() %>%
  mutate(obs = rpois(n(), I_det))

mdl <- lm(obs ~ D, data = df)

obs_plot <- ggplot(df, aes(x = D, y = obs)) +
  ggtheme_plot() +
  geom_abline(intercept = mdl$coefficients[1], slope = mdl$coefficients[2], color = 'blue') +
  geom_line(aes(y = I_det), color = 'red') +
  geom_point(size = 2) +
  labs(x = 'True abundance',
       y = 'Observed')

obs_plot
```

## Pseudocode 7.5

To calc neg log likelihood of a given observation, start with likelihood $\L$:

$$\L(\lambda | k = obs) = P(k = obs | \lambda) = \frac{\lambda^{k} e^{-\lambda}}{k!}$$
then neg log likelihood:
$$\LL\{\lambda | k\} = -(-\lambda \ln(e) + k \ln(\lambda) - \ln(k!))$$
$$= \lambda - k\ln(\lambda)  + \ln(k!)$$
In this, $\lambda$ is our predicted value $I_{det}$ for each model.  Since the last term is independent of $\lambda$, we could just drop it - it is the same for each model, so drops out in the math later because we're looking at differences between the models.  But we can just calculate it and keep it in, why not?

``` {r 7.5}

q <- 1
r <- 0.03
p <- -3

nsims <- 200

best_model <- vector(length = nsims)

for(sim in 1:nsims) { ### sim <- 1
  
  ### to avoid singular results, let's use a while loop, creating a
  ### new set of observations until the nls() models are NOT singular
  singular <- TRUE
  while(singular) {
    ### generate a new random set of observations
    obs_df <- data.frame(D = 1:20) %>%
      rowwise() %>%
      mutate(I_det = max(0, (p + q * D) / (1 + r * D))) %>%
      ungroup() %>%
      mutate(obs = rpois(n(), I_det))
    
    ### generate params for each model
    singular <- FALSE
    mdl_a_ls <- lm(obs ~ 0 + D, data = obs_df) %>% coefficients
    mdl_b_ls <- lm(obs ~ 1 + D, data = obs_df) %>% coefficients
    mdl_c_ls <- tryCatch(expr = {nls(obs ~ (a * D)/(1 + b * D), 
                                      start = c('a' = p, 'b' = r), 
                                      data = obs_df) %>% coefficients},
             error = function(e) TRUE)
    mdl_d_ls <- tryCatch(expr = {nls(obs ~ (w + a * D)/(1 + b * D),
                                     start = c('w' = p, 'a' = q, 'b' = r),
                                     data = obs_df) %>% coefficients},
             error = function(e) TRUE)
    singular <- any(mdl_c_ls == TRUE, mdl_d_ls == TRUE)
  }
  ### generate df for each model
  mdl_a_df <- data.frame(D = 1:20) %>%
    rowwise() %>%
    mutate(I_det = max(0, mdl_a_ls[1] * D),
           model = 'A') %>%
    ungroup()
  mdl_b_df <- data.frame(D = 1:20) %>%
    rowwise() %>%
    mutate(I_det = max(0, mdl_b_ls[1] + mdl_b_ls[2] * D),
           model = 'B') %>%
    ungroup()
  mdl_c_df <- data.frame(D = 1:20) %>%
    rowwise() %>%
    mutate(I_det = max(0, mdl_c_ls[1] * D / (1 + mdl_c_ls[2] * D)),
           model = 'C') %>%
    ungroup()
  mdl_d_df <- data.frame(D = 1:20) %>%
    rowwise() %>%
    mutate(I_det = max(0, (mdl_d_ls[1] + mdl_d_ls[2] * D) / (1 + mdl_d_ls[3] * D)),
           model = 'D') %>%
    ungroup()
  
  ### bind models and add column of observed values
  mdl_df <- bind_rows(mdl_a_df, mdl_b_df, mdl_c_df, mdl_d_df) %>%
    mutate(obs = rep(obs_df$obs, 4)) %>%
    mutate(nll = I_det - obs * log(I_det) + log(factorial(obs)) )
  
  mdl_compare <- mdl_df %>%
    group_by(model) %>%
    summarize(sum_nll = sum(nll, na.rm = TRUE))
  nll_vec <- mdl_compare$sum_nll %>% setNames(mdl_compare$model)
  
  ### a little testing to figure out which model is the best... ugh.  
  ### For each model test, compare the model to the others using a likelihood
  ### ratio test.  As set up, each named vector element is a likelihood
  ### ratio test against the variable in the object name.  E.g. a_test['C']
  ### is the result of a likelihood ratio test between models A and C,
  ### with a TRUE indicating A came out ahead and FALSE indicating that A
  ### did not come out ahead.
  ### So, if A came out ahead against all three other models, then A is the
  ### winner.  If not, go to b_test, and if B came out ahead of C and D (since
  ### we already tested it against A), then B is the winner.  Then to C,
  ### against D, and if none of the other tests was a clear winner, then D
  ### wins.
  a_test <- c('B' = 2 * (nll_vec['A'] - nll_vec['B']) < qchisq(.95, df = 1), ### 1 dof
              'C' = 2 * (nll_vec['A'] - nll_vec['C']) < qchisq(.95, df = 1), ### 1 dof
              'D' = 2 * (nll_vec['A'] - nll_vec['D']) < qchisq(.95, df = 2)) ### 2 dof
  b_test <- c('C' = nll_vec['B'] < nll_vec['C'],
              'D' = 2 * (nll_vec['B'] - nll_vec['D']) < qchisq(.95, df = 1)) ### 1 dof
  c_test <- 2 * (nll_vec['C'] - nll_vec['D']) < qchisq(.95, df = 1) ### 1 dof
  best_model[sim] <- case_when(all(a_test) ~ 'A',
                               all(b_test) ~ 'B',
                               all(c_test) ~ 'C',
                               TRUE        ~ 'D')
}

best_model <- factor(best_model, levels = c('A', 'B', 'C', 'D'))
  ### convert to factor, with level C included, so we see C even if C = 0

table(best_model)

```

## Pseudocode 7.6: infested nuts

Sampling pistachios to determine the proportion infested.  If 10% or fewer out of population are infested, then we're good.  With true infestation rate $f$, and on a sample size $S$, infested count $I$ in sample follows binomial distribution:
$$\Pr(I = i|f) = \binom{S}{i} f^i (1-f)^{S - i}$$

And neg log likelihood becomes
$$\LL(S, i |f) = -i \log f - (S - i) log(1-f) + J$$
Setting derivative of neg log likelihood with respect to f:
$$\frac{\partial \LL}{\partial f} = -\frac{i}{f} + \frac{S - i}{1 - f} = 0$$
$$\Rightarrow i - if = fS - if \Rightarrow f_{MLE} = \frac{i}{S}$$

OK now that the infrastructure is out of the way... Find 95% confidence interval by looking at log likelihood profile, and all values with a likelihood of 1.92 of MLE estimator.
$$\LL(S, i | f) - \LL(f_{MLE} | S, i) = 1.92$$
just a positive check, since the first term will always be higher NLL than the second, since $\LL(f_{MLE}|S, i)$ should be the minimum of a convex profile.  Besides, we are looking for the upper bound.

Pseudocode suggests a sequential sampling technique - sampling 20 at a time, until we reach an upper bound $f_b$ (upper 95% confidence interval) that is 0.10 or less. Why sequential?  we can just sample a ton, but if we do a few smaller samples, maybe we can stop sampling earlier and go home.

Let's run a few simulations, each with different random samples, and see how many iterations it takes on average to achieve an acceptable target threshold $f_{b,95\%}$ for the given $f, S$ values.

``` {r} 
# set.seed(12345) ### set seed for reproducibility

n_sims <- 1000
sims_list <- vector('list', n_sims)

f <- 0.04 
  ### set true infestation rate; has to be *below* threshold
  ### to provide reasonable certainty of reaching a 95% conf int
  ### that is still below our threshold for acceptance.
f_b_thresh <- 0.10 ### set acceptance threshold
samp_size <- 20 ### number chosen per sample

for(sim in 1:n_sims) { ### sim <- 1
  
  S_tot <- 0 ### initialize S and i total values
  i_tot <- 0
  f_b   <- 1 ### initialize boundary infestation rate well above threshold
  j     <- 0 ### initialize index for counting samples
  
  while(f_b > f_b_thresh) { ### 
    j <- j + 1
    I <- rbinom(n = 1, size = samp_size, prob = f)
    S_tot <- S_tot + samp_size
    i_tot <- i_tot + I
    
    f_MLE <- i_tot/S_tot
    
    if(f_MLE == 0) next()
      ### if f_MLE == 0, then we run into errors with logs.
    
    L_f_MLE <- -i_tot * log(f_MLE) - (S_tot - i_tot) * log(1 - f_MLE)
      ### drop J term since cancels in difference
    L_f <- L_f_MLE + 1.92
      ### this is our target; solve for f that meets this value.
    
    ### How to solve for f?  can do by plugging in a bunch of f values
    ### and calculating L_f from the -i log(f) - (S - i) log(1 - f)
    ### ... otherwise some ugly log math
    
    ### set a vector from f_MLE up to 1, knowing that we are looking
    ### for the bound ABOVE f_MLE.
    f_vec <- seq(f_MLE, 1, .001)
  
    L_f_vec <- -i_tot * log(f_vec) - (S_tot - i_tot) * log(1 - f_vec)
    f_b_indx <- min(which(L_f_vec > L_f))
      ### We want the first value that *exceeds* the NLL(f_MLE) + 1.92
      ### (to be)
    
    f_b <- f_vec[f_b_indx]
    
    if(j > 1000) break()
      ### to avoid an infinite loop...
    
    tmp_df <- data.frame(sim = sim,
                         samples = j,
                         S_total = S_tot,
                         I_total = i_tot,
                         f_MLE = f_MLE,
                         f_b = f_b)
  } ### end of iterations for this sim
  
  sims_list[[sim]] <- tmp_df
    ### don't forget double brackets, dammit
}
### Setting f (true value) to 0.10 gave an unreasonably large
### number of iterations to converge, or got stuck in an infinite loop.
### What value did the book code set? Closer to 0.04 (4 infected
### out of 100 samples)

sims_df <- bind_rows(sims_list)

ggplot(sims_df, aes(x = samples)) +
  ggtheme_plot() +
  geom_histogram(binwidth = 1, center = 0,
                 color = 'grey30', fill = 'grey50') +
  xlim(0, NA) +
  labs(x = paste0('Number of samples (sample size = ', samp_size, ')'),
       y = paste0('frequency out of ', n_sims, ' simulations'))

# DT::datatable(sims_df)

```

## Pseudocode 7.7, jesus does this chapter ever end?

Going back to our abundance index eqn 7.43:
$$I = \max\left(0, \frac{p + qD}{1 + rD}\right)$$
We want to find the likelihood profile for $q$, by finding the values of $p,r$ that maximize likelihood for each possible value of $q$.  The text and figure 7.9 are probably based on values from 7.5: $q = 1, p = -3, r = .03$.

Neg log likelihood (from pseudocode 7.5 above):
$$\LL = \lambda - k\ln(\lambda)  + \ln(k!)$$
In this, $\lambda$ is our predicted value $I_{det}$ for each model, and $k$ is our $I_{obs}$. However, the $\log()$ term causes issues when $\lambda \leq 0$.  From the numerator of the $I_{det}$, $p + qD$, we can tell that this will happen when $p$ is negative and product $qD$ is small (i.e. $q$ is low, and/or $D$ is low).

So: calculate NLL by only summing non-NA terms.  This seems problematic... and this problem is apparent in jumpiness of the plot at low $q$ values, since we're comparing sums with different numbers of terms.  Could create a false impression of the min of one of those jumps is large (i.e. a high number of NAs).  Other possibilities - gapfill NAs with a mean value, or just take the mean (rather than the sum) of the non-NA terms?

In this version I changed p to be positive, to unambiguously avoid this whole problem.

``` {r pseudocode 7.7 part A}
### generate observation data frame: same as in 7.4
q <- 1; r <- .03; p <- 3

D_vec <- 1:20

obs_df <- data.frame(D = D_vec) %>%
  rowwise() %>%
  mutate(I_det = max(0, (p + q * D) / (1 + r * D))) %>%
  ungroup() %>%
  mutate(obs = rpois(n(), I_det))

### option a: use true values of p and r
q_vec <- seq(.1, 5, .05)
nll_vec_a <- vector('numeric', length(q_vec))
for(i in seq_along(q_vec)) { ### i <- 1
  q_test <- q_vec[i]
  p_test <- p; r_test <- r
  
  nll_df <- obs_df %>%
    rowwise() %>%
    mutate(lambda = (p_test + q_test * D) / (1 + r_test * D),
           nll = lambda - obs * log(lambda) + log(factorial(obs))) %>%
    ungroup()
  
  nll_vec_a[i] <- sum(nll_df$nll, na.rm = TRUE)
}

# plot(q_vec, nll_vec_a)
```

``` {r pseudocode 7.7 part B}
### Option B: instead of using known p and r, search for best p and r for
### this value of q_test.  We probably don't need to know what those optimal
### p and r values are...
q_vec <- seq(.1, 5, .05)

p_vec <- seq(0, 6, .25)    ### range around p = -3
r_vec <- seq(-.05, +.15, .01) ### range around r = 0.03

### initialize vector of NLL values for each element of q_vec
nll_vec_b <- vector('numeric', length(q_vec))

for(i in seq_along(q_vec)) { ### i <- 1
  q_test <- q_vec[i]
  message('q_test: ', q_test, '... ' )
  
  nll_min <- Inf ### initialize a very high NLL for this value of q_test
  for(j in seq_along(p_vec)) {
    p_test <- p_vec[j]
    for(k in seq_along(r_vec)) {
      r_test <- r_vec[k]
      
      nll_df <- obs_df %>%
        rowwise() %>%
        mutate(lambda = (p_test + q_test * D) / (1 + r_test * D),
               nll = lambda - obs * log(lambda) + log(factorial(obs))) %>%
        ungroup()
      
      nll_sum <- sum(nll_df$nll, na.rm = TRUE)
      
      if(nll_sum < nll_min) {
        ### this combo of p_test and r_test creates a lower NLL than prior
        ### combos, so this is now the lowest.
        nll_min <- nll_sum
      }
    } ### end of r_vec loop
  }   ### end of p_vec loop
  
  nll_vec_b[i] <- nll_min
  
} ### end of q_vec loop

# plot(q_vec, nll_vec_b)

```

``` {r plot the two results}

nll_df <- data.frame(q = q_vec,
                 a = nll_vec_a,
                 b = nll_vec_b) %>%
  gather(test, NLL, a, b) %>%
  filter(NLL < 100)
ggplot(nll_df, aes(x = q, y = NLL, color = test)) +
  ggtheme_plot() +
  geom_point() +
  geom_line()

```

This doesn't look all that much like Figure 7.8... but test A ($r$ and $p$ fixed at actual values) stays narrow compared to finding optimal $r$ and $p$ for each $q$.  Perhaps the pseudocode intends us to use the same data as in Table 7.1?  but I generated new random data for this anyway.

## Pseudocode 7.8 - apparently, no

The pseudocode says to use data from Table 7.1, but we will just generate random data.

We will assume $p = 0, r = 0$ and bootstrap to find estimates of $q$... though the data is not based on these values of $p, r$... so we run it again with true values and see how it does.  In both cases, I use the mean, rather than the sum, to help avoid issues where we get artificially low NLL values b/c lots of NAs.

### using r = p = 0

``` {r pseudocode 7.8}
### generate observation data frame: same as in 7.4
q <- 1; r <- .03; p <- 3

D_vec <- 1:20

obs_df <- data.frame(D = D_vec) %>%
  rowwise() %>%
  mutate(I_det = max(0, (p + q * D) / (1 + r * D))) %>%
  ungroup() %>%
  mutate(obs = rpois(n(), I_det))

sims <- 1000

p_test <- 0; r_test <- 0
q_vec <- seq(.5, 1.5, .025)

q_est_vec <- rep(NA, sims)

for(sim in 1:sims) { ### sim <- 1
  if(sim%%25 == 0) message('bootstrap sim: ', sim)
  bs_df <- sample_n(obs_df, size = nrow(obs_df), replace = TRUE)
  
  nll_vec_bs <- vector('numeric', length(q_vec))
  for(i in seq_along(q_vec)) { ### i <- 2
    q_test <- q_vec[i]

    nll_df <- bs_df %>%
      rowwise() %>%
      mutate(lambda = (p_test + q_test * D) / (1 + r_test * D),
             nll = lambda - obs * log(lambda) + log(factorial(obs))) %>%
      ungroup()
    
    nll_vec_bs[i] <- mean(nll_df$nll, na.rm = TRUE)
  }
  q_est_vec[sim] <- q_vec[which(nll_vec_bs == min(nll_vec_bs))]
}

bs_results_df <- data.frame(sim = 1:sims,
                            q_est = q_est_vec)

ggplot(bs_results_df, aes(x = q_est)) +
  geom_histogram(binwidth = 0.025, color = 'grey30', fill = 'cyan3')
```

### using known r and p

``` {r try again using known parameters r and p}

p_test <- p; r_test <- r
q_vec <- seq(.5, 1.5, .025)

q_est_vec <- rep(NA, sims)

for(sim in 1:sims) { ### sim <- 1
  if(sim%%25 == 0) message('bootstrap sim: ', sim)
  bs_df <- sample_n(obs_df, size = nrow(obs_df), replace = TRUE)
  
  nll_vec_bs <- vector('numeric', length(q_vec))
  for(i in seq_along(q_vec)) { ### i <- 2
    q_test <- q_vec[i]

    nll_df <- bs_df %>%
      rowwise() %>%
      mutate(lambda = (p_test + q_test * D) / (1 + r_test * D),
             nll = lambda - obs * log(lambda) + log(factorial(obs))) %>%
      ungroup()
    
    nll_vec_bs[i] <- mean(nll_df$nll, na.rm = TRUE)
  }
  q_est_vec[sim] <- q_vec[which(nll_vec_bs == min(nll_vec_bs))]
}

bs_results_df <- data.frame(sim = 1:sims,
                            q_est = q_est_vec)

ggplot(bs_results_df, aes(x = q_est)) +
  geom_histogram(binwidth = 0.025, color = 'grey30', fill = 'cyan3') 

```


## Some code ideas to try later

``` {r, eval = FALSE}
# r_K_t_df<- tidyr::crossing(r = r_vec, K = K_vec, t = t_vec)

### https://stackoverflow.com/questions/38983532/dplyr-iterative-calculation
### In general, if you want to calculate values that rely on previous values, 
### you are better off using Reduce. here's an example with your data
### 
### x %>% mutate(x3 = Reduce(function(a,b) a*1.1+1, 1:(n()-1), 0, acc=T))
### But in your example, there is a closed form for the term that doesn't 
### rely on iteration. You can do
### 
### x %>% mutate(x4=(1.1^(row_number()-1)-1)/(1.1-1)*1)


# x <- data.frame(x1 = c(1:10)) 
# 
# #This works.
# x$x2[1] <- 0
# 
# for (i in 2:nrow(x)) {
#   x$x2[i] <- x$x2[i-1]*1.1 + 1
# }
# 
# x <- x %>% mutate(x3 = Reduce(function(a, b) a*1.1+1, 1:(n()-1), 0, acc=T))


# library(dplyr)
# library(magrittr)
# x <- data.frame(x1 = c(1:10))
# f_x <- function(x){(x-1)*1.1+1}
# x$x2 <-x %$% x1 %>% f_x
#     
#     
# N_det <- function(N_lag, r, K, harvest) {
#   N = N_lag + r * N_lag * (1 - N_lag / K) - harvest * N_lag
# }
# 
# N_df <- r_K_t_df %>%
#   group_by(r, K) %>%
#   mutate(harvest = ifelse(t %in% 3:5, 0.50, 0.01)) %>%
#   mutate(N = Map(f = N_det, r, K, harvest))



```


